{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: Arial, sans-serif; color:#97f788\">Ordinal Classification</span>\n",
    "\n",
    "<span style=\"font-family: Arial, sans-serif; color:navyblue\">Author: <a href=\"https://github.com/deburky\" title=\"GitHub link\">https://github.com/deburky</a></span>\n",
    "\n",
    "This notebook tracks some experiments with simple ordinal classification approach (SOCA) proposed by Eibe Frank and Mark Hall.\n",
    "\n",
    "I owe the discovery of this to @mosh98. You can find their implementation [here](https://github.com/mosh98/Ordinal_Classifier).\n",
    "Corresponding Medium post is provided [here](https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c).\n",
    "\n",
    "### Simple Ordinal Classification Approach\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"https://media.springernature.com/w316/springer-static/cover-hires/book/978-3-540-44795-5?as=webp\" width=\"700\" height=\"400\" alt=\"Book cover image\"></td>\n",
    "<td>\n",
    "\n",
    "The target we model each class as 1 if target > k, 0 otherwise. This results in several classifiers that are used in an ensemble manner. The first and last classifier are used as direct probability, and classes in-between are modeled as the difference between the probabilities of the next class.\n",
    "\n",
    "For example we model the probability of class 1 out of 5 classes as:\n",
    "- $1 - P(target > 1) = P(target <= 1)$\n",
    "\n",
    "For the last class:\n",
    "- $P(target > 4) = P(target = 5)$\n",
    "\n",
    "The classes in between can be modeled either as:\n",
    "- $P(target > k) - P(target > k+1)$ (approach in the scikit-learn implementation by @mosh98)\n",
    "- $P(target > k) Ã— (1-P(target > k+1))$ (original approach, p. 148)\n",
    "\n",
    "Both approaches work well, but the original approach showed better results on our dataset.\n",
    "\n",
    "**Reference:**\n",
    "> Eibe Frank and Mark Hall. A Simple Approach to Ordinal Classification. Machine Learning: ECML 2001. Lecture Notes in Computer Science, vol 2167. Springer, Berlin, Heidelberg. [Link](https://link.springer.com/chapter/10.1007/3-540-44795-4_13)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "The dataset used is [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews) with OpenAI embeddings ([repo](https://github.com/openai/openai-cookbook)), which are used as features (1536 embeddings per review) and use the score (1-5) as our target variable.\n",
    "\n",
    "To test the approach we use:\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- One-layer Neural Network\n",
    "\n",
    "To illustrate how models fit the embedding space, we visualize Fisher information on a reduced embedding space using t-SNE/PCA.\n",
    "\n",
    "The idea of ordinal classification is similar to proportional odds logistic regression model as described in Applied Logistic Regression (3rd ed.) by David W. Hosmer, Stanley Lemeshow, and Rodney X. Sturdivant.\n",
    "\n",
    "Ordinal classification is a more generic approach, since we can use probabilistic classifiers other than logistic regression.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "There are several ways to perform ordinal classification, most based on logistic regression methodologies.\n",
    "\n",
    "The scikit-learn implementation or ordinal classifier comes from this [repo](https://github.com/mosh98/Ordinal_Classifier/tree/master), which implements the simple ordinal classification approach.\n",
    "\n",
    "Additionally, `spacecutter` library by Ethan Rosenthal is tested for PyTorch models.\n",
    "\n",
    "You can read more about the PyTorch approach [here](https://www.ethanrosenthal.com/2018/12/06/spacecutter-ordinal-regression/) as well as find the [repo](https://github.com/EthanRosenthal/spacecutter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from rich import print as rprint\n",
    "\n",
    "datafile_path = (\n",
    "    \"data/fine_food_reviews_with_embeddings_1k.parquet\"\n",
    ")\n",
    "\n",
    "df = pd.read_parquet(datafile_path)\n",
    "df[\"embedding\"] = df.embedding.apply(literal_eval).apply(\n",
    "    np.array # type: ignore\n",
    ")  # type: ignore # convert string to array\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values),\n",
    "    df.Score,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# train a logistic regression\n",
    "clf_open = LogisticRegression(\n",
    "    fit_intercept=True,\n",
    "    solver=\"newton-cg\",\n",
    "    penalty=None,\n",
    "    random_state=42,\n",
    "    multi_class=\"multinomial\",\n",
    ")\n",
    "\n",
    "clf_open.fit(X_train, y_train)\n",
    "preds = clf_open.predict(X_test)\n",
    "probas = clf_open.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(\n",
    "    y_test, preds, output_dict=False\n",
    ")\n",
    "rprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Union\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# Visualizer of model performance\n",
    "def plot_multiclass_roc_auc(\n",
    "    y_score: Union[pd.Series, pd.DataFrame],\n",
    "    y_true_untransformed: Union[pd.Series, pd.DataFrame],\n",
    "    class_list: List[str],\n",
    "    classifier_name: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    ROC AUC plotting for a multiclass problem.\n",
    "    It plots the ROC curve and computes the AUC for each class,\n",
    "    as well as the micro-average AUC.\n",
    "\n",
    "    \"\"\"\n",
    "    n_classes = len(class_list)\n",
    "    y_true = pd.concat(\n",
    "        [\n",
    "            (y_true_untransformed == class_list[i])\n",
    "            for i in range(n_classes)\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).values\n",
    "\n",
    "    # choose cmap tab10 from matplotlib\n",
    "    colors = plt.get_cmap(\"Set2\").colors\n",
    "\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "\n",
    "    # Compute ROC curve and ROC AUC for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(\n",
    "            y_true[:, i],\n",
    "            y_score[:, i],\n",
    "            drop_intermediate=False,\n",
    "        )\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC AUC\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(\n",
    "        y_true.ravel(), y_score.ravel()\n",
    "    )\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    print(\n",
    "        f\"{classifier_name} - Micro-average Gini: {roc_auc['micro'] * 2 - 1:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(5, 5), dpi=100)\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f'Micro-Avg={roc_auc[\"micro\"] * 2 - 1:.2f}',\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            lw=2,\n",
    "            label=f\"Class {class_list[i]}={roc_auc[i] * 2 - 1:.2f}\",\n",
    "            color=colors[i],\n",
    "        )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"1 - TNR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"ROC curves\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_roc_auc(\n",
    "    probas, y_test, [1, 2, 3, 4, 5], clf_open\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacecutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from spacecutter.models import OrdinalLogisticModel\n",
    "from skorch import NeuralNet\n",
    "from spacecutter.callbacks import AscensionCallback\n",
    "from spacecutter.losses import CumulativeLinkLoss\n",
    "\n",
    "df_ord = df.copy()\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df_ord.embedding.values),\n",
    "    df_ord.Score - 1,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.float32).copy()\n",
    "y_train = np.array(y_train).reshape(-1, 1).copy()\n",
    "\n",
    "# Determine the number of features and classes\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Number of unique classes: {num_classes}\")\n",
    "\n",
    "# Feedforward Neural Network\n",
    "predictor = nn.Sequential(\n",
    "    nn.Linear(num_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "model = OrdinalLogisticModel(predictor, num_classes)\n",
    "\n",
    "ordinal_regression = NeuralNet(\n",
    "    module=model,\n",
    "    module__predictor=predictor,\n",
    "    module__num_classes=num_classes,\n",
    "    criterion=CumulativeLinkLoss,\n",
    "    lr=0.2,\n",
    "    max_epochs=100,\n",
    "    train_split=None,\n",
    "    callbacks=[\n",
    "        ('ascension', AscensionCallback()),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ordinal_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "probas_ord = ordinal_regression.predict_proba(\n",
    "    np.array(X_test, dtype=np.float32)\n",
    ").flatten()\n",
    "\n",
    "preds_ord = ordinal_regression.predict(\n",
    "    np.array(X_test, dtype=np.float32)\n",
    ").argmax(axis=-1)\n",
    "\n",
    "probas_ord_reshaped = probas_ord.reshape(-1, num_classes)\n",
    "\n",
    "probas = pd.DataFrame(\n",
    "    probas_ord_reshaped, \n",
    "    columns=[f'prob_class_{i}' for i in range(num_classes)]\n",
    ").to_numpy()\n",
    "\n",
    "_ = plot_multiclass_roc_auc(\n",
    "    probas, y_test, [0, 1, 2, 3, 4], ordinal_regression\n",
    ")\n",
    "\n",
    "report = classification_report(y_test.values, preds_ord)\n",
    "rprint(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Classification\n",
    "\n",
    "[Source](https://github.com/mosh98/Ordinal_Classifier/blob/master/Ordinal_Classifier.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from ordinal_classifier import OrdinalClassifier\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "datafile_path = (\n",
    "    \"data/fine_food_reviews_with_embeddings_1k.parquet\"\n",
    ")\n",
    "\n",
    "df = pd.read_parquet(datafile_path)\n",
    "df[\"embedding\"] = df.embedding.apply(literal_eval).apply(\n",
    "    np.array\n",
    ")  # type: ignore\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values),\n",
    "    df.Score - 1,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "X_train_, X_test_ = np.array(X_train, dtype=np.float16), np.array(X_test, dtype=np.float16)\n",
    "y_train_, y_test_ = (\n",
    "    np.array(y_train).reshape(-1, 1).ravel(), \n",
    "    np.array(y_test).reshape(-1, 1).ravel()\n",
    ")\n",
    "\n",
    "clf_base = svm.SVC(\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "clf = OrdinalClassifier(clf_base)\n",
    "\n",
    "clf.fit(X_train_, y_train_)\n",
    "preds_ord_clf = clf.predict(X_test_)\n",
    "probas_ord_clf = clf.predict_proba(X_test_)\n",
    "\n",
    "report = classification_report(\n",
    "    y_test, preds_ord_clf, output_dict=False\n",
    ")\n",
    "console.print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison (Softmax vs Ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ordinal_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "\n",
    "    total_count = len(y_true)\n",
    "\n",
    "    accurate_count = sum(\n",
    "        1\n",
    "        for true_label, pred_label in zip(y_true, y_pred)\n",
    "        if pred_label in [true_label, true_label - 1, true_label + 1]\n",
    "    )\n",
    "    return accurate_count / total_count\n",
    "\n",
    "\n",
    "# Assuming y_test and preds are already defined\n",
    "preds_lr = clf_open.predict(X_test)\n",
    "preds_svm = clf.predict(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "accuracy_log_reg = ordinal_accuracy(y_test, preds_lr)\n",
    "print(f\"Ordinal Accuracy for Logistic Regression: {accuracy_log_reg:.2%}\")\n",
    "\n",
    "# Ordinal SVM Model\n",
    "accuracy_ordinal_svm = ordinal_accuracy(y_test, preds_svm)\n",
    "print(f\"Ordinal Accuracy for Ordinal SVM: {accuracy_ordinal_svm:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher information\n",
    "\n",
    "The calculation of Fisher Information as implemented in the code is based on the gradients of the log-likelihood with respect to the parameters. This method is typically considered an observed Fisher Information since it is computed from the actual data points.\n",
    "\n",
    "In the code, the Fisher Information is computed as the sum of the squares of the gradients of the log-likelihood. This is a common approach to estimating the observed Fisher Information:\n",
    "\n",
    "```python\n",
    "for i in range(log_likelihoods.shape[1]):\n",
    "    zz = log_likelihoods[:, i].reshape(xx.shape)\n",
    "    gx, gy = np.gradient(zz)\n",
    "    fim[:, :, i] = gx**2 + gy**2\n",
    "```\n",
    "\n",
    "Here, gx and gy are the gradients of the log-likelihoods with respect to the two dimensions of the t-SNE space. The Fisher Information is then estimated as gx**2 + gy**2.\n",
    "\n",
    "Similar to the 2D case, the Fisher Information is calculated as the sum of the squares of the gradients in the 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fisher_information_visualizer import FisherInformationVisualizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_base = LogisticRegression(max_iter=1000, solver='newton-cg', C=10.0, random_state=42)\n",
    "clf_base = SVC(probability=True, random_state=42)\n",
    "# clf_base = MLPClassifier(hidden_layer_sizes=(100, 10, ), max_iter=1000, random_state=42, learning_rate_init=1e-2)\n",
    "\n",
    "visualizer = FisherInformationVisualizer(clf_base, X_train, y_train.ravel(), X_test, y_test, method='t-SNE')\n",
    "# visualizer = FisherInformationVisualizer(clf_base, X_train, y_train, X_test, y_test, method='PCA')\n",
    "visualizer.plot_2d_fisher_information(dimensionality=100, contour_levels=100)\n",
    "visualizer.plot_3d_fisher_information(dimensionality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-project-nVfOrddR-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
