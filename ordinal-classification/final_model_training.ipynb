{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Ordinal Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-24 19:40:01,792] A new study created in memory with name: no-name-aee96d4c-3a57-43d6-8c15-aa786d280c47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings in the dataframe: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:40:08,654] Trial 0 finished with value: 0.7964154715341156 and parameters: {'nhead': 3, 'd_model': 192, 'num_encoder_layers': 5, 'dim_feedforward': 664, 'learning_rate': 2.9380279387035334e-05, 'batch_size': 64, 'num_epochs': 39}. Best is trial 0 with value: 0.7964154715341156.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:40:10,416] Trial 1 finished with value: 0.8048618921745949 and parameters: {'nhead': 1, 'd_model': 64, 'num_encoder_layers': 5, 'dim_feedforward': 318, 'learning_rate': 3.511356313970405e-05, 'batch_size': 64, 'num_epochs': 21}. Best is trial 1 with value: 0.8048618921745949.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:40:13,398] Trial 2 finished with value: 0.8016993269508568 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 2, 'dim_feedforward': 456, 'learning_rate': 0.00023345864076016249, 'batch_size': 10, 'num_epochs': 11}. Best is trial 1 with value: 0.8048618921745949.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:40:18,000] Trial 3 finished with value: 0.8238473411995871 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 1, 'dim_feedforward': 979, 'learning_rate': 0.00788671412999049, 'batch_size': 10, 'num_epochs': 28}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:40:20,433] Trial 4 finished with value: 0.7986780616120333 and parameters: {'nhead': 1, 'd_model': 32, 'num_encoder_layers': 1, 'dim_feedforward': 943, 'learning_rate': 5.9750279999602906e-05, 'batch_size': 10, 'num_epochs': 17}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:41:07,211] Trial 5 finished with value: 0.5955528228507444 and parameters: {'nhead': 8, 'd_model': 448, 'num_encoder_layers': 6, 'dim_feedforward': 930, 'learning_rate': 0.0006218704727769079, 'batch_size': 10, 'num_epochs': 23}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:41:12,639] Trial 6 finished with value: 0.796724680081823 and parameters: {'nhead': 4, 'd_model': 96, 'num_encoder_layers': 5, 'dim_feedforward': 448, 'learning_rate': 6.963114377829287e-05, 'batch_size': 64, 'num_epochs': 50}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:41:13,647] Trial 7 finished with value: 0.8014742018127409 and parameters: {'nhead': 7, 'd_model': 112, 'num_encoder_layers': 1, 'dim_feedforward': 859, 'learning_rate': 0.001319994226153501, 'batch_size': 32, 'num_epochs': 14}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:41:15,332] Trial 8 finished with value: 0.8000012958166233 and parameters: {'nhead': 7, 'd_model': 280, 'num_encoder_layers': 2, 'dim_feedforward': 185, 'learning_rate': 8.569331925053983e-05, 'batch_size': 128, 'num_epochs': 29}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:41:17,567] Trial 9 finished with value: 0.6741339031339031 and parameters: {'nhead': 1, 'd_model': 48, 'num_encoder_layers': 5, 'dim_feedforward': 631, 'learning_rate': 0.002055424552015075, 'batch_size': 32, 'num_epochs': 14}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:41:20,503] Trial 10 finished with value: 0.7507321982760579 and parameters: {'nhead': 5, 'd_model': 160, 'num_encoder_layers': 3, 'dim_feedforward': 764, 'learning_rate': 0.008947232156563659, 'batch_size': 128, 'num_epochs': 37}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:41:22,255] Trial 11 finished with value: 0.7596648159942403 and parameters: {'nhead': 3, 'd_model': 72, 'num_encoder_layers': 4, 'dim_feedforward': 228, 'learning_rate': 1.0687807249245659e-05, 'batch_size': 64, 'num_epochs': 25}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:41:33,618] Trial 12 finished with value: 0.5935969348659004 and parameters: {'nhead': 3, 'd_model': 24, 'num_encoder_layers': 4, 'dim_feedforward': 360, 'learning_rate': 0.006525361957534156, 'batch_size': 10, 'num_epochs': 34}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:41:39,023] Trial 13 finished with value: 0.7972732426303856 and parameters: {'nhead': 6, 'd_model': 288, 'num_encoder_layers': 6, 'dim_feedforward': 315, 'learning_rate': 0.0002647573637067341, 'batch_size': 64, 'num_epochs': 21}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:41:48,478] Trial 14 finished with value: 0.8013441352676317 and parameters: {'nhead': 2, 'd_model': 64, 'num_encoder_layers': 3, 'dim_feedforward': 501, 'learning_rate': 1.1930220880472078e-05, 'batch_size': 10, 'num_epochs': 30}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:41:51,792] Trial 15 finished with value: 0.7886259078199664 and parameters: {'nhead': 4, 'd_model': 128, 'num_encoder_layers': 2, 'dim_feedforward': 1016, 'learning_rate': 0.0031468010727186252, 'batch_size': 64, 'num_epochs': 46}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:41:54,071] Trial 16 finished with value: 0.8042533193996609 and parameters: {'nhead': 6, 'd_model': 240, 'num_encoder_layers': 4, 'dim_feedforward': 724, 'learning_rate': 0.0005872127889434882, 'batch_size': 128, 'num_epochs': 20}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:41:56,254] Trial 17 finished with value: 0.8000184918441294 and parameters: {'nhead': 2, 'd_model': 48, 'num_encoder_layers': 3, 'dim_feedforward': 133, 'learning_rate': 0.00014369814714873508, 'batch_size': 32, 'num_epochs': 27}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:42:01,652] Trial 18 finished with value: 0.8045477532377614 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 5, 'dim_feedforward': 318, 'learning_rate': 2.585764685675642e-05, 'batch_size': 64, 'num_epochs': 33}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-24 19:42:07,109] Trial 19 finished with value: 0.7977515247474903 and parameters: {'nhead': 2, 'd_model': 48, 'num_encoder_layers': 1, 'dim_feedforward': 539, 'learning_rate': 0.0006511737431075875, 'batch_size': 10, 'num_epochs': 40}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 1, 'dim_feedforward': 979, 'learning_rate': 0.00788671412999049, 'batch_size': 10, 'num_epochs': 28}\n",
      "Best score: 0.8238\n",
      "Epoch 1/28, Loss: 0.0243\n",
      "Epoch 2/28, Loss: 0.0129\n",
      "Epoch 3/28, Loss: 0.1351\n",
      "Epoch 4/28, Loss: 0.0232\n",
      "Epoch 5/28, Loss: 0.0108\n",
      "Epoch 6/28, Loss: 0.3519\n",
      "Epoch 7/28, Loss: 0.0088\n",
      "Epoch 8/28, Loss: 0.0066\n",
      "Epoch 9/28, Loss: 0.0590\n",
      "Epoch 10/28, Loss: 0.0032\n",
      "Epoch 11/28, Loss: 0.0456\n",
      "Epoch 12/28, Loss: 0.1001\n",
      "Epoch 13/28, Loss: 0.0065\n",
      "Epoch 14/28, Loss: 0.1043\n",
      "Epoch 15/28, Loss: 0.0309\n",
      "Epoch 16/28, Loss: 0.0360\n",
      "Epoch 17/28, Loss: 0.0063\n",
      "Epoch 18/28, Loss: 0.1100\n",
      "Epoch 19/28, Loss: 0.0068\n",
      "Epoch 20/28, Loss: 0.0075\n",
      "Epoch 21/28, Loss: 0.0116\n",
      "Epoch 22/28, Loss: 0.0193\n",
      "Epoch 23/28, Loss: 0.2012\n",
      "Epoch 24/28, Loss: 0.0055\n",
      "Epoch 25/28, Loss: 0.1198\n",
      "Epoch 26/28, Loss: 0.1229\n",
      "Epoch 27/28, Loss: 0.0048\n",
      "Epoch 28/28, Loss: 0.0228\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.73      0.61      0.67        31\n",
      "     Class 1       0.55      0.35      0.43        17\n",
      "     Class 2       0.43      0.62      0.51        16\n",
      "     Class 3       0.69      0.58      0.63        43\n",
      "     Class 4       0.91      0.96      0.93       193\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.66      0.63      0.63       300\n",
      "weighted avg       0.81      0.82      0.81       300\n",
      "\n",
      "Ordinal accuracy of the network on the test data: 97%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def ordinal_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    total_count = len(y_true)\n",
    "    accurate_count = sum(\n",
    "        1\n",
    "        for true_label, pred_label in zip(y_true, y_pred)\n",
    "        if pred_label in [true_label, true_label - 1, true_label + 1]\n",
    "    )\n",
    "    return accurate_count / total_count\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "datafile_path = \"../data/fine_food_reviews_fine_tuned_e5_small_v2_1k.parquet\"\n",
    "df = pd.read_parquet(datafile_path)\n",
    "\n",
    "embedding_dim = np.array(list(df.embedding.values)).shape[1]\n",
    "print(f\"Shape of embeddings in the dataframe: {embedding_dim}\")\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values),\n",
    "    df.Score - 1,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "n_classes = len(y_train.unique())\n",
    "\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embeddings[idx].clone().detach().float()\n",
    "        label = self.labels[idx].clone().detach().long()\n",
    "        return embedding, label\n",
    "\n",
    "\n",
    "# Convert train and test splits to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach()\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long).clone().detach()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).clone().detach()\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long).clone().detach()\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = ReviewsDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = ReviewsDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": embedding_dim,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"num_hidden_1\": 256,\n",
    "    \"num_hidden_2\": 128,\n",
    "    \"num_hidden_3\": 64,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"num_epochs\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        dim_feedforward,\n",
    "        output_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer_encoder(x.unsqueeze(1))  # Add sequence dimension\n",
    "        x = self.fc(x.squeeze(1))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the loss function for CORN\n",
    "def loss_corn(logits, y_train, num_classes):\n",
    "    sets = []\n",
    "    for i in range(num_classes - 1):\n",
    "        label_mask = y_train > i - 1\n",
    "        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "        sets.append((label_mask, label_tensor))\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.0\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "\n",
    "        loss = -torch.sum(\n",
    "            F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels)\n",
    "        )\n",
    "        losses += loss\n",
    "    return losses / num_examples\n",
    "\n",
    "\n",
    "def label_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels. This function is specific to CORN.\"\"\"\n",
    "    probas = torch.sigmoid(logits)\n",
    "    probas = torch.cumprod(probas, dim=1)\n",
    "    predict_levels = probas > 0.5\n",
    "    return torch.sum(predict_levels, dim=1)\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    nhead = trial.suggest_int(\"nhead\", 1, 8)\n",
    "    d_model = trial.suggest_int(\"d_model\", nhead * 8, nhead * 64, step=nhead * 8)\n",
    "    num_encoder_layers = trial.suggest_int(\"num_encoder_layers\", 1, 6)\n",
    "    dim_feedforward = trial.suggest_int(\"dim_feedforward\", 128, 1024)\n",
    "    # dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 32, 64, 128])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 10, 50)\n",
    "\n",
    "    # Create DataLoader objects with the new batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create model\n",
    "    model = TransformerModel(\n",
    "        input_size=embedding_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        output_size=n_classes - 1,\n",
    "    ).to(params[\"device\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, targets in train_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "\n",
    "            # CORN loss\n",
    "            loss = loss_corn(logits, targets, n_classes)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            logits = model(features)\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "            preds = label_from_logits(logits)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    return f1_score(all_targets, all_preds, average=\"weighted\")\n",
    "\n",
    "\n",
    "# Run the Optuna optimization\n",
    "tpe_sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=tpe_sampler)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_trial.value\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "\n",
    "# Update the params dictionary with the best hyperparameters\n",
    "params.update(best_params)\n",
    "\n",
    "# Create DataLoader objects with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Create and train the final model with the best hyperparameters\n",
    "model = TransformerModel(\n",
    "    input_size=params[\"input_dim\"],\n",
    "    d_model=params[\"d_model\"],\n",
    "    nhead=params[\"nhead\"],\n",
    "    num_encoder_layers=params[\"num_encoder_layers\"],\n",
    "    dim_feedforward=params[\"dim_feedforward\"],\n",
    "    output_size=params[\"n_classes\"] - 1,\n",
    ").to(params[\"device\"])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    for features, targets in train_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "\n",
    "        # CORN loss\n",
    "        loss = loss_corn(logits, targets, params[\"n_classes\"])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        logits = model(features)\n",
    "        preds = label_from_logits(logits)\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\n",
    "    classification_report(\n",
    "        all_targets, all_preds, target_names=[f\"Class {i}\" for i in range(n_classes)]\n",
    "    )\n",
    ")\n",
    "\n",
    "ordinal_accuracy_score = ordinal_accuracy(np.array(all_targets), np.array(all_preds))\n",
    "print(f\"Ordinal accuracy of the network on the test data: {ordinal_accuracy_score:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9d93f_row0_col0, #T_9d93f_row0_col1, #T_9d93f_row0_col2, #T_9d93f_row0_col3, #T_9d93f_row1_col0, #T_9d93f_row1_col1, #T_9d93f_row1_col2, #T_9d93f_row1_col3, #T_9d93f_row3_col0, #T_9d93f_row3_col1, #T_9d93f_row3_col2, #T_9d93f_row3_col3, #T_9d93f_row4_col0, #T_9d93f_row4_col1, #T_9d93f_row4_col2, #T_9d93f_row4_col3, #T_9d93f_row6_col0, #T_9d93f_row6_col1, #T_9d93f_row7_col0, #T_9d93f_row7_col1, #T_9d93f_row7_col2, #T_9d93f_row7_col3, #T_9d93f_row8_col0, #T_9d93f_row8_col1, #T_9d93f_row8_col2, #T_9d93f_row9_col0, #T_9d93f_row9_col4, #T_9d93f_row9_col5 {\n",
       "  background-color: #5da2ff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_9d93f_row0_col4, #T_9d93f_row0_col5, #T_9d93f_row1_col4, #T_9d93f_row1_col5, #T_9d93f_row3_col4, #T_9d93f_row3_col5, #T_9d93f_row4_col4, #T_9d93f_row4_col5, #T_9d93f_row7_col4, #T_9d93f_row7_col5 {\n",
       "  background-color: #a25dff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_9d93f_row2_col0, #T_9d93f_row2_col1, #T_9d93f_row2_col2, #T_9d93f_row2_col3, #T_9d93f_row2_col4, #T_9d93f_row2_col5, #T_9d93f_row5_col0, #T_9d93f_row5_col1, #T_9d93f_row5_col2, #T_9d93f_row5_col3, #T_9d93f_row5_col4, #T_9d93f_row5_col5, #T_9d93f_row6_col2, #T_9d93f_row6_col3, #T_9d93f_row8_col3, #T_9d93f_row9_col1, #T_9d93f_row9_col2, #T_9d93f_row9_col3 {\n",
       "  background-color: #45baff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_9d93f_row6_col4, #T_9d93f_row6_col5 {\n",
       "  background-color: #748bff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_9d93f_row8_col4, #T_9d93f_row8_col5 {\n",
       "  background-color: #8b74ff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9d93f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9d93f_level0_col0\" class=\"col_heading level0 col0\" >Score 2</th>\n",
       "      <th id=\"T_9d93f_level0_col1\" class=\"col_heading level0 col1\" >Score 3</th>\n",
       "      <th id=\"T_9d93f_level0_col2\" class=\"col_heading level0 col2\" >Score 4</th>\n",
       "      <th id=\"T_9d93f_level0_col3\" class=\"col_heading level0 col3\" >Score 5</th>\n",
       "      <th id=\"T_9d93f_level0_col4\" class=\"col_heading level0 col4\" >True label</th>\n",
       "      <th id=\"T_9d93f_level0_col5\" class=\"col_heading level0 col5\" >Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row0\" class=\"row_heading level0 row0\" >220</th>\n",
       "      <td id=\"T_9d93f_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_9d93f_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_9d93f_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_9d93f_row0_col4\" class=\"data row0 col4\" >4</td>\n",
       "      <td id=\"T_9d93f_row0_col5\" class=\"data row0 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row1\" class=\"row_heading level0 row1\" >221</th>\n",
       "      <td id=\"T_9d93f_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_9d93f_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_9d93f_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "      <td id=\"T_9d93f_row1_col4\" class=\"data row1 col4\" >4</td>\n",
       "      <td id=\"T_9d93f_row1_col5\" class=\"data row1 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row2\" class=\"row_heading level0 row2\" >222</th>\n",
       "      <td id=\"T_9d93f_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_9d93f_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_9d93f_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_9d93f_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_9d93f_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "      <td id=\"T_9d93f_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row3\" class=\"row_heading level0 row3\" >223</th>\n",
       "      <td id=\"T_9d93f_row3_col0\" class=\"data row3 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "      <td id=\"T_9d93f_row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "      <td id=\"T_9d93f_row3_col3\" class=\"data row3 col3\" >1</td>\n",
       "      <td id=\"T_9d93f_row3_col4\" class=\"data row3 col4\" >4</td>\n",
       "      <td id=\"T_9d93f_row3_col5\" class=\"data row3 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row4\" class=\"row_heading level0 row4\" >224</th>\n",
       "      <td id=\"T_9d93f_row4_col0\" class=\"data row4 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "      <td id=\"T_9d93f_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "      <td id=\"T_9d93f_row4_col3\" class=\"data row4 col3\" >1</td>\n",
       "      <td id=\"T_9d93f_row4_col4\" class=\"data row4 col4\" >4</td>\n",
       "      <td id=\"T_9d93f_row4_col5\" class=\"data row4 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row5\" class=\"row_heading level0 row5\" >225</th>\n",
       "      <td id=\"T_9d93f_row5_col0\" class=\"data row5 col0\" >0</td>\n",
       "      <td id=\"T_9d93f_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_9d93f_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_9d93f_row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "      <td id=\"T_9d93f_row5_col4\" class=\"data row5 col4\" >0</td>\n",
       "      <td id=\"T_9d93f_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row6\" class=\"row_heading level0 row6\" >226</th>\n",
       "      <td id=\"T_9d93f_row6_col0\" class=\"data row6 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "      <td id=\"T_9d93f_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_9d93f_row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "      <td id=\"T_9d93f_row6_col4\" class=\"data row6 col4\" >2</td>\n",
       "      <td id=\"T_9d93f_row6_col5\" class=\"data row6 col5\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row7\" class=\"row_heading level0 row7\" >227</th>\n",
       "      <td id=\"T_9d93f_row7_col0\" class=\"data row7 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row7_col1\" class=\"data row7 col1\" >1</td>\n",
       "      <td id=\"T_9d93f_row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "      <td id=\"T_9d93f_row7_col3\" class=\"data row7 col3\" >1</td>\n",
       "      <td id=\"T_9d93f_row7_col4\" class=\"data row7 col4\" >4</td>\n",
       "      <td id=\"T_9d93f_row7_col5\" class=\"data row7 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row8\" class=\"row_heading level0 row8\" >228</th>\n",
       "      <td id=\"T_9d93f_row8_col0\" class=\"data row8 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row8_col1\" class=\"data row8 col1\" >1</td>\n",
       "      <td id=\"T_9d93f_row8_col2\" class=\"data row8 col2\" >1</td>\n",
       "      <td id=\"T_9d93f_row8_col3\" class=\"data row8 col3\" >0</td>\n",
       "      <td id=\"T_9d93f_row8_col4\" class=\"data row8 col4\" >3</td>\n",
       "      <td id=\"T_9d93f_row8_col5\" class=\"data row8 col5\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d93f_level0_row9\" class=\"row_heading level0 row9\" >229</th>\n",
       "      <td id=\"T_9d93f_row9_col0\" class=\"data row9 col0\" >1</td>\n",
       "      <td id=\"T_9d93f_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_9d93f_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_9d93f_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "      <td id=\"T_9d93f_row9_col4\" class=\"data row9 col4\" >1</td>\n",
       "      <td id=\"T_9d93f_row9_col5\" class=\"data row9 col5\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x105bb4850>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model analysis\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def label_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    probas = sigmoid(logits)\n",
    "    probas = np.cumprod(probas, axis=1)\n",
    "    predict_levels = probas > 0.5\n",
    "    return np.sum(predict_levels, axis=1)\n",
    "\n",
    "def probas_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    probas = sigmoid(logits)\n",
    "    probas_cum = np.cumprod(probas, axis=1)\n",
    "    return (probas_cum > 0.5).astype(int)\n",
    "\n",
    "names_for_columns = ['Score 2', 'Score 3', 'Score 4', 'Score 5']\n",
    "logits_df = pd.DataFrame(all_logits)\n",
    "preds_df = pd.DataFrame(label_from_logits(all_logits))\n",
    "probas_df = pd.DataFrame(probas_from_logits(all_logits)).rename(\n",
    "    columns=dict(enumerate(names_for_columns))\n",
    ")\n",
    "labels_df = pd.DataFrame({'True label': all_targets, 'Prediction': all_preds})\n",
    "\n",
    "slice_window = slice(220, 230)\n",
    "df_to_show = pd.concat(\n",
    "    [\n",
    "        probas_df[slice_window],\n",
    "        labels_df[slice_window]['True label'], \n",
    "        labels_df[slice_window]['Prediction']\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "df_to_show.style.background_gradient(axis=None, low=0.75, high=1.0, cmap='cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Multi-Class Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-24 19:45:22,718] A new study created in memory with name: no-name-10b37049-22e1-4275-8c42-9ffb8bff7111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings in the dataframe: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:45:29,472] Trial 0 finished with value: 0.7872796764253056 and parameters: {'nhead': 3, 'd_model': 192, 'num_encoder_layers': 5, 'dim_feedforward': 664, 'learning_rate': 2.9380279387035334e-05, 'batch_size': 64, 'num_epochs': 39}. Best is trial 0 with value: 0.7872796764253056.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:45:31,509] Trial 1 finished with value: 0.7959241760428201 and parameters: {'nhead': 1, 'd_model': 64, 'num_encoder_layers': 5, 'dim_feedforward': 318, 'learning_rate': 3.511356313970405e-05, 'batch_size': 64, 'num_epochs': 21}. Best is trial 1 with value: 0.7959241760428201.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:45:34,558] Trial 2 finished with value: 0.7986780616120333 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 2, 'dim_feedforward': 456, 'learning_rate': 0.00023345864076016249, 'batch_size': 10, 'num_epochs': 11}. Best is trial 2 with value: 0.7986780616120333.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:45:38,761] Trial 3 finished with value: 0.8077103184614994 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 1, 'dim_feedforward': 979, 'learning_rate': 0.00788671412999049, 'batch_size': 10, 'num_epochs': 28}. Best is trial 3 with value: 0.8077103184614994.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:45:40,563] Trial 4 finished with value: 0.8050499263626291 and parameters: {'nhead': 1, 'd_model': 32, 'num_encoder_layers': 1, 'dim_feedforward': 943, 'learning_rate': 5.9750279999602906e-05, 'batch_size': 10, 'num_epochs': 17}. Best is trial 3 with value: 0.8077103184614994.\n",
      "[I 2024-07-24 19:46:26,806] Trial 5 finished with value: 0.5389634228640842 and parameters: {'nhead': 8, 'd_model': 448, 'num_encoder_layers': 6, 'dim_feedforward': 930, 'learning_rate': 0.0006218704727769079, 'batch_size': 10, 'num_epochs': 23}. Best is trial 3 with value: 0.8077103184614994.\n",
      "[I 2024-07-24 19:46:32,397] Trial 6 finished with value: 0.8018039993788327 and parameters: {'nhead': 4, 'd_model': 96, 'num_encoder_layers': 5, 'dim_feedforward': 448, 'learning_rate': 6.963114377829287e-05, 'batch_size': 64, 'num_epochs': 50}. Best is trial 3 with value: 0.8077103184614994.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:46:33,243] Trial 7 finished with value: 0.8201964741957728 and parameters: {'nhead': 7, 'd_model': 112, 'num_encoder_layers': 1, 'dim_feedforward': 859, 'learning_rate': 0.001319994226153501, 'batch_size': 32, 'num_epochs': 14}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:46:35,122] Trial 8 finished with value: 0.7936297922624865 and parameters: {'nhead': 7, 'd_model': 280, 'num_encoder_layers': 2, 'dim_feedforward': 185, 'learning_rate': 8.569331925053983e-05, 'batch_size': 128, 'num_epochs': 29}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:46:37,292] Trial 9 finished with value: 0.8094912799549933 and parameters: {'nhead': 1, 'd_model': 48, 'num_encoder_layers': 5, 'dim_feedforward': 631, 'learning_rate': 0.002055424552015075, 'batch_size': 32, 'num_epochs': 14}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:46:44,652] Trial 10 finished with value: 0.8010648061519137 and parameters: {'nhead': 7, 'd_model': 224, 'num_encoder_layers': 3, 'dim_feedforward': 767, 'learning_rate': 0.0015919518669560474, 'batch_size': 32, 'num_epochs': 39}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:46:45,733] Trial 11 finished with value: 0.791474334106871 and parameters: {'nhead': 3, 'd_model': 24, 'num_encoder_layers': 4, 'dim_feedforward': 753, 'learning_rate': 0.00406786241776069, 'batch_size': 32, 'num_epochs': 11}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-24 19:46:48,695] Trial 12 finished with value: 0.8198206749788994 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 4, 'dim_feedforward': 591, 'learning_rate': 0.0014011005935128135, 'batch_size': 32, 'num_epochs': 16}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-24 19:46:51,495] Trial 13 finished with value: 0.8029378423356134 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 3, 'dim_feedforward': 820, 'learning_rate': 0.0006333741850055093, 'batch_size': 32, 'num_epochs': 19}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-24 19:47:00,319] Trial 14 finished with value: 0.8076843415197057 and parameters: {'nhead': 8, 'd_model': 384, 'num_encoder_layers': 4, 'dim_feedforward': 501, 'learning_rate': 0.0003095209069239348, 'batch_size': 32, 'num_epochs': 24}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-24 19:47:02,099] Trial 15 finished with value: 0.8153698942074995 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 2, 'dim_feedforward': 852, 'learning_rate': 0.0013099976221506787, 'batch_size': 128, 'num_epochs': 36}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-24 19:47:04,445] Trial 16 finished with value: 0.6215903903903904 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 3, 'dim_feedforward': 682, 'learning_rate': 0.009665161671817576, 'batch_size': 32, 'num_epochs': 16}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-24 19:47:06,914] Trial 17 finished with value: 0.6083957690249825 and parameters: {'nhead': 7, 'd_model': 280, 'num_encoder_layers': 4, 'dim_feedforward': 337, 'learning_rate': 0.0033833894359985594, 'batch_size': 32, 'num_epochs': 10}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-24 19:47:14,213] Trial 18 finished with value: 0.7964154715341156 and parameters: {'nhead': 4, 'd_model': 192, 'num_encoder_layers': 6, 'dim_feedforward': 549, 'learning_rate': 1.3053237253672277e-05, 'batch_size': 32, 'num_epochs': 26}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-24 19:47:15,644] Trial 19 finished with value: 0.8140264894771185 and parameters: {'nhead': 8, 'd_model': 384, 'num_encoder_layers': 1, 'dim_feedforward': 353, 'learning_rate': 0.0006975465142208243, 'batch_size': 128, 'num_epochs': 35}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'nhead': 7, 'd_model': 112, 'num_encoder_layers': 1, 'dim_feedforward': 859, 'learning_rate': 0.001319994226153501, 'batch_size': 32, 'num_epochs': 14}\n",
      "Best score: 0.8202\n",
      "Epoch 1/14, Loss: 0.0788\n",
      "Epoch 2/14, Loss: 0.3042\n",
      "Epoch 3/14, Loss: 0.5500\n",
      "Epoch 4/14, Loss: 0.1436\n",
      "Epoch 5/14, Loss: 0.2163\n",
      "Epoch 6/14, Loss: 0.0419\n",
      "Epoch 7/14, Loss: 0.0436\n",
      "Epoch 8/14, Loss: 0.0427\n",
      "Epoch 9/14, Loss: 0.0407\n",
      "Epoch 10/14, Loss: 0.0434\n",
      "Epoch 11/14, Loss: 0.0928\n",
      "Epoch 12/14, Loss: 0.0404\n",
      "Epoch 13/14, Loss: 0.2447\n",
      "Epoch 14/14, Loss: 0.3591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.55      0.71        31\n",
      "     Class 1       0.68      0.76      0.72        17\n",
      "     Class 2       0.42      0.62      0.50        16\n",
      "     Class 3       0.69      0.58      0.63        43\n",
      "     Class 4       0.91      0.96      0.93       193\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.74      0.70      0.70       300\n",
      "weighted avg       0.85      0.83      0.83       300\n",
      "\n",
      "Ordinal accuracy of the network on the test data: 96%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def ordinal_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    total_count = len(y_true)\n",
    "    accurate_count = sum(\n",
    "        1\n",
    "        for true_label, pred_label in zip(y_true, y_pred)\n",
    "        if pred_label in [true_label, true_label - 1, true_label + 1]\n",
    "    )\n",
    "    return accurate_count / total_count\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "datafile_path = \"../data/fine_food_reviews_fine_tuned_e5_small_v2_1k.parquet\"\n",
    "df = pd.read_parquet(datafile_path)\n",
    "\n",
    "embedding_dim = np.array(list(df.embedding.values)).shape[1]\n",
    "print(f\"Shape of embeddings in the dataframe: {embedding_dim}\")\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values),\n",
    "    df.Score - 1,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "n_classes = len(y_train.unique())\n",
    "\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embeddings[idx].clone().detach().float()\n",
    "        label = self.labels[idx].clone().detach().long()\n",
    "        return embedding, label\n",
    "\n",
    "\n",
    "# Convert train and test splits to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach()\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long).clone().detach()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).clone().detach()\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long).clone().detach()\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = ReviewsDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = ReviewsDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": embedding_dim,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"num_hidden_1\": 256,\n",
    "    \"num_hidden_2\": 128,\n",
    "    \"num_hidden_3\": 64,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"num_epochs\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        dim_feedforward,\n",
    "        output_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer_encoder(x.unsqueeze(1))  # Add sequence dimension\n",
    "        x = self.fc(x.squeeze(1))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    nhead = trial.suggest_int(\"nhead\", 1, 8)\n",
    "    d_model = trial.suggest_int(\"d_model\", nhead * 8, nhead * 64, step=nhead * 8)\n",
    "    num_encoder_layers = trial.suggest_int(\"num_encoder_layers\", 1, 6)\n",
    "    dim_feedforward = trial.suggest_int(\"dim_feedforward\", 128, 1024)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 32, 64, 128])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 10, 50)\n",
    "\n",
    "    # Create DataLoader objects with the new batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create model\n",
    "    model = TransformerModel(\n",
    "        input_size=embedding_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        output_size=n_classes,\n",
    "    ).to(params[\"device\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, targets in train_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            logits = model(features)\n",
    "            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    return f1_score(all_targets, all_preds, average=\"weighted\")\n",
    "\n",
    "\n",
    "# Run the Optuna optimization\n",
    "tpe_sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=tpe_sampler)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_trial.value\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "\n",
    "# Update the params dictionary with the best hyperparameters\n",
    "params.update(best_params)\n",
    "\n",
    "# Create DataLoader objects with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Create and train the final model with the best hyperparameters\n",
    "model = TransformerModel(\n",
    "    input_size=params[\"input_dim\"],\n",
    "    d_model=params[\"d_model\"],\n",
    "    nhead=params[\"nhead\"],\n",
    "    num_encoder_layers=params[\"num_encoder_layers\"],\n",
    "    dim_feedforward=params[\"dim_feedforward\"],\n",
    "    output_size=params[\"n_classes\"],\n",
    ").to(params[\"device\"])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    for features, targets in train_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        logits = model(features)\n",
    "        preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\n",
    "    classification_report(\n",
    "        all_targets, all_preds, target_names=[f\"Class {i}\" for i in range(n_classes)]\n",
    "    )\n",
    ")\n",
    "\n",
    "ordinal_accuracy_score = ordinal_accuracy(np.array(all_targets), np.array(all_preds))\n",
    "print(f\"Ordinal accuracy of the network on the test data: {ordinal_accuracy_score:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_af894_row0_col0, #T_af894_row0_col1, #T_af894_row0_col2, #T_af894_row0_col3, #T_af894_row1_col0, #T_af894_row1_col1, #T_af894_row1_col3, #T_af894_row1_col4, #T_af894_row2_col0, #T_af894_row2_col1, #T_af894_row2_col2, #T_af894_row2_col3, #T_af894_row3_col0, #T_af894_row3_col1, #T_af894_row3_col2, #T_af894_row3_col3, #T_af894_row4_col0, #T_af894_row4_col1, #T_af894_row4_col2, #T_af894_row4_col3, #T_af894_row5_col1, #T_af894_row5_col2, #T_af894_row5_col3, #T_af894_row5_col4, #T_af894_row5_col5, #T_af894_row5_col6, #T_af894_row6_col0, #T_af894_row6_col2, #T_af894_row6_col3, #T_af894_row6_col4, #T_af894_row6_col5, #T_af894_row7_col0, #T_af894_row7_col1, #T_af894_row7_col2, #T_af894_row7_col3, #T_af894_row8_col0, #T_af894_row8_col1, #T_af894_row8_col2, #T_af894_row8_col4, #T_af894_row9_col0, #T_af894_row9_col1, #T_af894_row9_col2, #T_af894_row9_col3 {\n",
       "  background-color: #45baff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_af894_row0_col4, #T_af894_row1_col2, #T_af894_row1_col5, #T_af894_row2_col4, #T_af894_row3_col4, #T_af894_row4_col4, #T_af894_row5_col0, #T_af894_row6_col1, #T_af894_row6_col6, #T_af894_row7_col4, #T_af894_row8_col3, #T_af894_row9_col4 {\n",
       "  background-color: #5da2ff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_af894_row0_col5, #T_af894_row0_col6, #T_af894_row2_col5, #T_af894_row2_col6, #T_af894_row3_col5, #T_af894_row3_col6, #T_af894_row4_col5, #T_af894_row4_col6, #T_af894_row7_col6, #T_af894_row9_col5, #T_af894_row9_col6 {\n",
       "  background-color: #a25dff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_af894_row1_col6 {\n",
       "  background-color: #748bff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_af894_row7_col5, #T_af894_row8_col5, #T_af894_row8_col6 {\n",
       "  background-color: #8b74ff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_af894\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_af894_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_af894_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_af894_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_af894_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_af894_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_af894_level0_col5\" class=\"col_heading level0 col5\" >True label</th>\n",
       "      <th id=\"T_af894_level0_col6\" class=\"col_heading level0 col6\" >Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row0\" class=\"row_heading level0 row0\" >160</th>\n",
       "      <td id=\"T_af894_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_af894_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_af894_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_af894_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_af894_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "      <td id=\"T_af894_row0_col5\" class=\"data row0 col5\" >4</td>\n",
       "      <td id=\"T_af894_row0_col6\" class=\"data row0 col6\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row1\" class=\"row_heading level0 row1\" >161</th>\n",
       "      <td id=\"T_af894_row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "      <td id=\"T_af894_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_af894_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_af894_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_af894_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_af894_row1_col5\" class=\"data row1 col5\" >1</td>\n",
       "      <td id=\"T_af894_row1_col6\" class=\"data row1 col6\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row2\" class=\"row_heading level0 row2\" >162</th>\n",
       "      <td id=\"T_af894_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_af894_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_af894_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_af894_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_af894_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "      <td id=\"T_af894_row2_col5\" class=\"data row2 col5\" >4</td>\n",
       "      <td id=\"T_af894_row2_col6\" class=\"data row2 col6\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row3\" class=\"row_heading level0 row3\" >163</th>\n",
       "      <td id=\"T_af894_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "      <td id=\"T_af894_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_af894_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_af894_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "      <td id=\"T_af894_row3_col4\" class=\"data row3 col4\" >1</td>\n",
       "      <td id=\"T_af894_row3_col5\" class=\"data row3 col5\" >4</td>\n",
       "      <td id=\"T_af894_row3_col6\" class=\"data row3 col6\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row4\" class=\"row_heading level0 row4\" >164</th>\n",
       "      <td id=\"T_af894_row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "      <td id=\"T_af894_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_af894_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_af894_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "      <td id=\"T_af894_row4_col4\" class=\"data row4 col4\" >1</td>\n",
       "      <td id=\"T_af894_row4_col5\" class=\"data row4 col5\" >4</td>\n",
       "      <td id=\"T_af894_row4_col6\" class=\"data row4 col6\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row5\" class=\"row_heading level0 row5\" >165</th>\n",
       "      <td id=\"T_af894_row5_col0\" class=\"data row5 col0\" >1</td>\n",
       "      <td id=\"T_af894_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_af894_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_af894_row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "      <td id=\"T_af894_row5_col4\" class=\"data row5 col4\" >0</td>\n",
       "      <td id=\"T_af894_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_af894_row5_col6\" class=\"data row5 col6\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row6\" class=\"row_heading level0 row6\" >166</th>\n",
       "      <td id=\"T_af894_row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "      <td id=\"T_af894_row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "      <td id=\"T_af894_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_af894_row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "      <td id=\"T_af894_row6_col4\" class=\"data row6 col4\" >0</td>\n",
       "      <td id=\"T_af894_row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "      <td id=\"T_af894_row6_col6\" class=\"data row6 col6\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row7\" class=\"row_heading level0 row7\" >167</th>\n",
       "      <td id=\"T_af894_row7_col0\" class=\"data row7 col0\" >0</td>\n",
       "      <td id=\"T_af894_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_af894_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_af894_row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "      <td id=\"T_af894_row7_col4\" class=\"data row7 col4\" >1</td>\n",
       "      <td id=\"T_af894_row7_col5\" class=\"data row7 col5\" >3</td>\n",
       "      <td id=\"T_af894_row7_col6\" class=\"data row7 col6\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row8\" class=\"row_heading level0 row8\" >168</th>\n",
       "      <td id=\"T_af894_row8_col0\" class=\"data row8 col0\" >0</td>\n",
       "      <td id=\"T_af894_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_af894_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_af894_row8_col3\" class=\"data row8 col3\" >1</td>\n",
       "      <td id=\"T_af894_row8_col4\" class=\"data row8 col4\" >0</td>\n",
       "      <td id=\"T_af894_row8_col5\" class=\"data row8 col5\" >3</td>\n",
       "      <td id=\"T_af894_row8_col6\" class=\"data row8 col6\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af894_level0_row9\" class=\"row_heading level0 row9\" >169</th>\n",
       "      <td id=\"T_af894_row9_col0\" class=\"data row9 col0\" >0</td>\n",
       "      <td id=\"T_af894_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_af894_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_af894_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "      <td id=\"T_af894_row9_col4\" class=\"data row9 col4\" >1</td>\n",
       "      <td id=\"T_af894_row9_col5\" class=\"data row9 col5\" >4</td>\n",
       "      <td id=\"T_af894_row9_col6\" class=\"data row9 col6\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x353617550>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model analysis\n",
    "from scipy.special import softmax\n",
    "\n",
    "def label_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    z = softmax(logits, axis=1)\n",
    "    return z / z.sum(axis=1, keepdims=1)\n",
    "\n",
    "def probas_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    z = softmax(logits, axis=1)\n",
    "    softm_ = z / z.sum(axis=1, keepdims=1)\n",
    "    return (softm_ > 0.5).astype(int)\n",
    "\n",
    "logits_df = pd.DataFrame(all_logits)\n",
    "preds_df = pd.DataFrame(label_from_logits(all_logits))\n",
    "probas_df = pd.DataFrame(probas_from_logits(all_logits))\n",
    "labels_df = pd.DataFrame({'True label': all_targets, 'Prediction': all_preds})\n",
    "\n",
    "slice_window = slice(160, 170)\n",
    "df_to_show = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(probas_df[slice_window]), \n",
    "        labels_df[slice_window]['True label'], \n",
    "        labels_df[slice_window]['Prediction']\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "df_to_show.style.background_gradient(axis=None, low=0.75, high=1.0, cmap='cool')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lr-focal-loss-Opjerf94-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
