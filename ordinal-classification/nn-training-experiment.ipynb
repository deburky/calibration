{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Ordinal Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 23:02:08,426] A new study created in memory with name: no-name-4b193d95-463d-448c-94b6-be2f1be70073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings in the dataframe: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:02:15,586] Trial 0 finished with value: 0.7964154715341156 and parameters: {'nhead': 3, 'd_model': 192, 'num_encoder_layers': 5, 'dim_feedforward': 664, 'learning_rate': 2.9380279387035334e-05, 'batch_size': 64, 'num_epochs': 39}. Best is trial 0 with value: 0.7964154715341156.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:02:17,438] Trial 1 finished with value: 0.8048618921745949 and parameters: {'nhead': 1, 'd_model': 64, 'num_encoder_layers': 5, 'dim_feedforward': 318, 'learning_rate': 3.511356313970405e-05, 'batch_size': 64, 'num_epochs': 21}. Best is trial 1 with value: 0.8048618921745949.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:02:20,574] Trial 2 finished with value: 0.8016993269508568 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 2, 'dim_feedforward': 456, 'learning_rate': 0.00023345864076016249, 'batch_size': 10, 'num_epochs': 11}. Best is trial 1 with value: 0.8048618921745949.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:02:25,637] Trial 3 finished with value: 0.8238473411995871 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 1, 'dim_feedforward': 979, 'learning_rate': 0.00788671412999049, 'batch_size': 10, 'num_epochs': 28}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:02:27,941] Trial 4 finished with value: 0.7986780616120333 and parameters: {'nhead': 1, 'd_model': 32, 'num_encoder_layers': 1, 'dim_feedforward': 943, 'learning_rate': 5.9750279999602906e-05, 'batch_size': 10, 'num_epochs': 17}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:03:12,380] Trial 5 finished with value: 0.5955528228507444 and parameters: {'nhead': 8, 'd_model': 448, 'num_encoder_layers': 6, 'dim_feedforward': 930, 'learning_rate': 0.0006218704727769079, 'batch_size': 10, 'num_epochs': 23}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:03:18,809] Trial 6 finished with value: 0.796724680081823 and parameters: {'nhead': 4, 'd_model': 96, 'num_encoder_layers': 5, 'dim_feedforward': 448, 'learning_rate': 6.963114377829287e-05, 'batch_size': 64, 'num_epochs': 50}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:03:19,919] Trial 7 finished with value: 0.8014742018127409 and parameters: {'nhead': 7, 'd_model': 112, 'num_encoder_layers': 1, 'dim_feedforward': 859, 'learning_rate': 0.001319994226153501, 'batch_size': 32, 'num_epochs': 14}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:03:21,929] Trial 8 finished with value: 0.8000012958166233 and parameters: {'nhead': 7, 'd_model': 280, 'num_encoder_layers': 2, 'dim_feedforward': 185, 'learning_rate': 8.569331925053983e-05, 'batch_size': 128, 'num_epochs': 29}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:03:24,440] Trial 9 finished with value: 0.6741339031339031 and parameters: {'nhead': 1, 'd_model': 48, 'num_encoder_layers': 5, 'dim_feedforward': 631, 'learning_rate': 0.002055424552015075, 'batch_size': 32, 'num_epochs': 14}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:03:27,255] Trial 10 finished with value: 0.7507321982760579 and parameters: {'nhead': 5, 'd_model': 160, 'num_encoder_layers': 3, 'dim_feedforward': 764, 'learning_rate': 0.008947232156563659, 'batch_size': 128, 'num_epochs': 37}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:03:29,223] Trial 11 finished with value: 0.7596648159942403 and parameters: {'nhead': 3, 'd_model': 72, 'num_encoder_layers': 4, 'dim_feedforward': 228, 'learning_rate': 1.0687807249245659e-05, 'batch_size': 64, 'num_epochs': 25}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:03:41,225] Trial 12 finished with value: 0.5935969348659004 and parameters: {'nhead': 3, 'd_model': 24, 'num_encoder_layers': 4, 'dim_feedforward': 360, 'learning_rate': 0.006525361957534156, 'batch_size': 10, 'num_epochs': 34}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:03:46,423] Trial 13 finished with value: 0.7972732426303856 and parameters: {'nhead': 6, 'd_model': 288, 'num_encoder_layers': 6, 'dim_feedforward': 315, 'learning_rate': 0.0002647573637067341, 'batch_size': 64, 'num_epochs': 21}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:03:56,314] Trial 14 finished with value: 0.8013441352676317 and parameters: {'nhead': 2, 'd_model': 64, 'num_encoder_layers': 3, 'dim_feedforward': 501, 'learning_rate': 1.1930220880472078e-05, 'batch_size': 10, 'num_epochs': 30}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:04:00,405] Trial 15 finished with value: 0.7886259078199664 and parameters: {'nhead': 4, 'd_model': 128, 'num_encoder_layers': 2, 'dim_feedforward': 1016, 'learning_rate': 0.0031468010727186252, 'batch_size': 64, 'num_epochs': 46}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:04:02,661] Trial 16 finished with value: 0.8042533193996609 and parameters: {'nhead': 6, 'd_model': 240, 'num_encoder_layers': 4, 'dim_feedforward': 724, 'learning_rate': 0.0005872127889434882, 'batch_size': 128, 'num_epochs': 20}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:04:05,044] Trial 17 finished with value: 0.8000184918441294 and parameters: {'nhead': 2, 'd_model': 48, 'num_encoder_layers': 3, 'dim_feedforward': 133, 'learning_rate': 0.00014369814714873508, 'batch_size': 32, 'num_epochs': 27}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:04:09,261] Trial 18 finished with value: 0.8045477532377614 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 5, 'dim_feedforward': 318, 'learning_rate': 2.585764685675642e-05, 'batch_size': 64, 'num_epochs': 33}. Best is trial 3 with value: 0.8238473411995871.\n",
      "[I 2024-07-23 23:04:14,940] Trial 19 finished with value: 0.7977515247474903 and parameters: {'nhead': 2, 'd_model': 48, 'num_encoder_layers': 1, 'dim_feedforward': 539, 'learning_rate': 0.0006511737431075875, 'batch_size': 10, 'num_epochs': 40}. Best is trial 3 with value: 0.8238473411995871.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 1, 'dim_feedforward': 979, 'learning_rate': 0.00788671412999049, 'batch_size': 10, 'num_epochs': 28}\n",
      "Best score: 0.8238\n",
      "Epoch 1/28, Loss: 0.0243\n",
      "Epoch 2/28, Loss: 0.0129\n",
      "Epoch 3/28, Loss: 0.1351\n",
      "Epoch 4/28, Loss: 0.0232\n",
      "Epoch 5/28, Loss: 0.0108\n",
      "Epoch 6/28, Loss: 0.3519\n",
      "Epoch 7/28, Loss: 0.0088\n",
      "Epoch 8/28, Loss: 0.0066\n",
      "Epoch 9/28, Loss: 0.0590\n",
      "Epoch 10/28, Loss: 0.0032\n",
      "Epoch 11/28, Loss: 0.0456\n",
      "Epoch 12/28, Loss: 0.1001\n",
      "Epoch 13/28, Loss: 0.0065\n",
      "Epoch 14/28, Loss: 0.1043\n",
      "Epoch 15/28, Loss: 0.0309\n",
      "Epoch 16/28, Loss: 0.0360\n",
      "Epoch 17/28, Loss: 0.0063\n",
      "Epoch 18/28, Loss: 0.1100\n",
      "Epoch 19/28, Loss: 0.0068\n",
      "Epoch 20/28, Loss: 0.0075\n",
      "Epoch 21/28, Loss: 0.0116\n",
      "Epoch 22/28, Loss: 0.0193\n",
      "Epoch 23/28, Loss: 0.2012\n",
      "Epoch 24/28, Loss: 0.0055\n",
      "Epoch 25/28, Loss: 0.1198\n",
      "Epoch 26/28, Loss: 0.1229\n",
      "Epoch 27/28, Loss: 0.0048\n",
      "Epoch 28/28, Loss: 0.0228\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.73      0.61      0.67        31\n",
      "     Class 1       0.55      0.35      0.43        17\n",
      "     Class 2       0.43      0.62      0.51        16\n",
      "     Class 3       0.69      0.58      0.63        43\n",
      "     Class 4       0.91      0.96      0.93       193\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.66      0.63      0.63       300\n",
      "weighted avg       0.81      0.82      0.81       300\n",
      "\n",
      "Ordinal accuracy of the network on the test data: 97%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def ordinal_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    total_count = len(y_true)\n",
    "    accurate_count = sum(\n",
    "        1\n",
    "        for true_label, pred_label in zip(y_true, y_pred)\n",
    "        if pred_label in [true_label, true_label - 1, true_label + 1]\n",
    "    )\n",
    "    return accurate_count / total_count\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "datafile_path = \"../data/fine_food_reviews_fine_tuned_e5_small_v2_1k.parquet\"\n",
    "df = pd.read_parquet(datafile_path)\n",
    "\n",
    "embedding_dim = np.array(list(df.embedding.values)).shape[1]\n",
    "print(f\"Shape of embeddings in the dataframe: {embedding_dim}\")\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values),\n",
    "    df.Score - 1,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Convert the problem to three classes (very bad, bad, neutral-good)\n",
    "# y_train = y_train.apply(lambda x: 0 if x < 3 else 1 if x == 3 else 2)\n",
    "# y_test = y_test.apply(lambda x: 0 if x < 3 else 1 if x == 3 else 2)\n",
    "\n",
    "n_classes = len(y_train.unique())\n",
    "\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embeddings[idx].clone().detach().float()\n",
    "        label = self.labels[idx].clone().detach().long()\n",
    "        return embedding, label\n",
    "\n",
    "\n",
    "# Convert train and test splits to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach()\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long).clone().detach()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).clone().detach()\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long).clone().detach()\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = ReviewsDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = ReviewsDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": embedding_dim,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"num_hidden_1\": 256,\n",
    "    \"num_hidden_2\": 128,\n",
    "    \"num_hidden_3\": 64,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"num_epochs\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        dim_feedforward,\n",
    "        output_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer_encoder(x.unsqueeze(1))  # Add sequence dimension\n",
    "        x = self.fc(x.squeeze(1))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the loss function for CORN\n",
    "def loss_corn(logits, y_train, num_classes):\n",
    "    sets = []\n",
    "    for i in range(num_classes - 1):\n",
    "        label_mask = y_train > i - 1\n",
    "        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "        sets.append((label_mask, label_tensor))\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.0\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "\n",
    "        loss = -torch.sum(\n",
    "            F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels)\n",
    "        )\n",
    "        losses += loss\n",
    "    return losses / num_examples\n",
    "\n",
    "\n",
    "def label_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels. This function is specific to CORN.\"\"\"\n",
    "    probas = torch.sigmoid(logits)\n",
    "    probas = torch.cumprod(probas, dim=1)\n",
    "    predict_levels = probas > 0.5\n",
    "    return torch.sum(predict_levels, dim=1)\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    nhead = trial.suggest_int(\"nhead\", 1, 8)\n",
    "    d_model = trial.suggest_int(\"d_model\", nhead * 8, nhead * 64, step=nhead * 8)\n",
    "    num_encoder_layers = trial.suggest_int(\"num_encoder_layers\", 1, 6)\n",
    "    dim_feedforward = trial.suggest_int(\"dim_feedforward\", 128, 1024)\n",
    "    # dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 32, 64, 128])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 10, 50)\n",
    "\n",
    "    # Create DataLoader objects with the new batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create model\n",
    "    model = TransformerModel(\n",
    "        input_size=embedding_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        output_size=n_classes - 1,\n",
    "    ).to(params[\"device\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, targets in train_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "\n",
    "            # CORN loss\n",
    "            loss = loss_corn(logits, targets, n_classes)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            logits = model(features)\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "            preds = label_from_logits(logits)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    return f1_score(all_targets, all_preds, average=\"weighted\")\n",
    "\n",
    "\n",
    "# Run the Optuna optimization\n",
    "tpe_sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=tpe_sampler)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_trial.value\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "\n",
    "# Update the params dictionary with the best hyperparameters\n",
    "params.update(best_params)\n",
    "\n",
    "# Create DataLoader objects with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Create and train the final model with the best hyperparameters\n",
    "model = TransformerModel(\n",
    "    input_size=params[\"input_dim\"],\n",
    "    d_model=params[\"d_model\"],\n",
    "    nhead=params[\"nhead\"],\n",
    "    num_encoder_layers=params[\"num_encoder_layers\"],\n",
    "    dim_feedforward=params[\"dim_feedforward\"],\n",
    "    output_size=params[\"n_classes\"] - 1,\n",
    ").to(params[\"device\"])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    for features, targets in train_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "\n",
    "        # CORN loss\n",
    "        loss = loss_corn(logits, targets, params[\"n_classes\"])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        logits = model(features)\n",
    "        preds = label_from_logits(logits)\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\n",
    "    classification_report(\n",
    "        all_targets, all_preds, target_names=[f\"Class {i}\" for i in range(n_classes)]\n",
    "    )\n",
    ")\n",
    "\n",
    "ordinal_accuracy_score = ordinal_accuracy(np.array(all_targets), np.array(all_preds))\n",
    "print(f\"Ordinal accuracy of the network on the test data: {ordinal_accuracy_score:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model analysis\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def label_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    probas = sigmoid(logits)\n",
    "    probas = np.cumprod(probas, axis=1)\n",
    "    predict_levels = probas > 0.5\n",
    "    return np.sum(predict_levels, axis=1)\n",
    "\n",
    "def probas_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    probas = sigmoid(logits)\n",
    "    probas_cum = np.cumprod(probas, axis=1)\n",
    "    return (probas_cum > 0.5).astype(int)\n",
    "\n",
    "logits_df = pd.DataFrame(all_logits)\n",
    "preds_df = pd.DataFrame(label_from_logits(all_logits))\n",
    "probas_df = pd.DataFrame(probas_from_logits(all_logits))\n",
    "labels_df = pd.DataFrame({'True Label': all_targets, 'Predicted Label': all_preds})\n",
    "\n",
    "slice_window = slice(220, 230)\n",
    "df_to_show = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(probas_df[slice_window]), \n",
    "        labels_df[slice_window]['True Label'], \n",
    "        labels_df[slice_window]['Predicted Label']\n",
    "    ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  True Label  Predicted Label\n",
       "220  1  1  1  1           4                4\n",
       "221  1  1  1  1           4                4\n",
       "222  0  0  0  0           0                0\n",
       "223  1  1  1  1           4                4\n",
       "224  1  1  1  1           4                4\n",
       "225  0  0  0  0           0                0\n",
       "226  1  1  0  0           2                2\n",
       "227  1  1  1  1           4                4\n",
       "228  1  1  1  0           3                3\n",
       "229  1  0  0  0           1                1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Multi-Class Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 23:46:51,827] A new study created in memory with name: no-name-f9b1e370-900e-4bab-b6b0-dd45e7b6d21a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings in the dataframe: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:46:58,757] Trial 0 finished with value: 0.7872796764253056 and parameters: {'nhead': 3, 'd_model': 192, 'num_encoder_layers': 5, 'dim_feedforward': 664, 'learning_rate': 2.9380279387035334e-05, 'batch_size': 64, 'num_epochs': 39}. Best is trial 0 with value: 0.7872796764253056.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:47:00,326] Trial 1 finished with value: 0.7959241760428201 and parameters: {'nhead': 1, 'd_model': 64, 'num_encoder_layers': 5, 'dim_feedforward': 318, 'learning_rate': 3.511356313970405e-05, 'batch_size': 64, 'num_epochs': 21}. Best is trial 1 with value: 0.7959241760428201.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:47:02,655] Trial 2 finished with value: 0.7986780616120333 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 2, 'dim_feedforward': 456, 'learning_rate': 0.00023345864076016249, 'batch_size': 10, 'num_epochs': 11}. Best is trial 2 with value: 0.7986780616120333.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:47:06,647] Trial 3 finished with value: 0.8077103184614994 and parameters: {'nhead': 5, 'd_model': 80, 'num_encoder_layers': 1, 'dim_feedforward': 979, 'learning_rate': 0.00788671412999049, 'batch_size': 10, 'num_epochs': 28}. Best is trial 3 with value: 0.8077103184614994.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:47:08,792] Trial 4 finished with value: 0.8050499263626291 and parameters: {'nhead': 1, 'd_model': 32, 'num_encoder_layers': 1, 'dim_feedforward': 943, 'learning_rate': 5.9750279999602906e-05, 'batch_size': 10, 'num_epochs': 17}. Best is trial 3 with value: 0.8077103184614994.\n",
      "[I 2024-07-23 23:47:52,839] Trial 5 finished with value: 0.5389634228640842 and parameters: {'nhead': 8, 'd_model': 448, 'num_encoder_layers': 6, 'dim_feedforward': 930, 'learning_rate': 0.0006218704727769079, 'batch_size': 10, 'num_epochs': 23}. Best is trial 3 with value: 0.8077103184614994.\n",
      "[I 2024-07-23 23:47:58,832] Trial 6 finished with value: 0.8018039993788327 and parameters: {'nhead': 4, 'd_model': 96, 'num_encoder_layers': 5, 'dim_feedforward': 448, 'learning_rate': 6.963114377829287e-05, 'batch_size': 64, 'num_epochs': 50}. Best is trial 3 with value: 0.8077103184614994.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:47:59,847] Trial 7 finished with value: 0.8201964741957728 and parameters: {'nhead': 7, 'd_model': 112, 'num_encoder_layers': 1, 'dim_feedforward': 859, 'learning_rate': 0.001319994226153501, 'batch_size': 32, 'num_epochs': 14}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:48:01,631] Trial 8 finished with value: 0.7936297922624865 and parameters: {'nhead': 7, 'd_model': 280, 'num_encoder_layers': 2, 'dim_feedforward': 185, 'learning_rate': 8.569331925053983e-05, 'batch_size': 128, 'num_epochs': 29}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:48:03,619] Trial 9 finished with value: 0.8094912799549933 and parameters: {'nhead': 1, 'd_model': 48, 'num_encoder_layers': 5, 'dim_feedforward': 631, 'learning_rate': 0.002055424552015075, 'batch_size': 32, 'num_epochs': 14}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:48:12,168] Trial 10 finished with value: 0.8010648061519137 and parameters: {'nhead': 7, 'd_model': 224, 'num_encoder_layers': 3, 'dim_feedforward': 767, 'learning_rate': 0.0015919518669560474, 'batch_size': 32, 'num_epochs': 39}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:48:13,497] Trial 11 finished with value: 0.791474334106871 and parameters: {'nhead': 3, 'd_model': 24, 'num_encoder_layers': 4, 'dim_feedforward': 753, 'learning_rate': 0.00406786241776069, 'batch_size': 32, 'num_epochs': 11}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-23 23:48:16,959] Trial 12 finished with value: 0.8198206749788994 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 4, 'dim_feedforward': 591, 'learning_rate': 0.0014011005935128135, 'batch_size': 32, 'num_epochs': 16}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-23 23:48:20,614] Trial 13 finished with value: 0.8029378423356134 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 3, 'dim_feedforward': 820, 'learning_rate': 0.0006333741850055093, 'batch_size': 32, 'num_epochs': 19}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-23 23:48:29,913] Trial 14 finished with value: 0.8076843415197057 and parameters: {'nhead': 8, 'd_model': 384, 'num_encoder_layers': 4, 'dim_feedforward': 501, 'learning_rate': 0.0003095209069239348, 'batch_size': 32, 'num_epochs': 24}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-23 23:48:31,841] Trial 15 finished with value: 0.8153698942074995 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 2, 'dim_feedforward': 852, 'learning_rate': 0.0013099976221506787, 'batch_size': 128, 'num_epochs': 36}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-23 23:48:34,793] Trial 16 finished with value: 0.6215903903903904 and parameters: {'nhead': 6, 'd_model': 144, 'num_encoder_layers': 3, 'dim_feedforward': 682, 'learning_rate': 0.009665161671817576, 'batch_size': 32, 'num_epochs': 16}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-23 23:48:37,429] Trial 17 finished with value: 0.6083957690249825 and parameters: {'nhead': 7, 'd_model': 280, 'num_encoder_layers': 4, 'dim_feedforward': 337, 'learning_rate': 0.0033833894359985594, 'batch_size': 32, 'num_epochs': 10}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-23 23:48:45,251] Trial 18 finished with value: 0.7964154715341156 and parameters: {'nhead': 4, 'd_model': 192, 'num_encoder_layers': 6, 'dim_feedforward': 549, 'learning_rate': 1.3053237253672277e-05, 'batch_size': 32, 'num_epochs': 26}. Best is trial 7 with value: 0.8201964741957728.\n",
      "[I 2024-07-23 23:48:46,651] Trial 19 finished with value: 0.8140264894771185 and parameters: {'nhead': 8, 'd_model': 384, 'num_encoder_layers': 1, 'dim_feedforward': 353, 'learning_rate': 0.0006975465142208243, 'batch_size': 128, 'num_epochs': 35}. Best is trial 7 with value: 0.8201964741957728.\n",
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/lr-focal-loss-Opjerf94-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'nhead': 7, 'd_model': 112, 'num_encoder_layers': 1, 'dim_feedforward': 859, 'learning_rate': 0.001319994226153501, 'batch_size': 32, 'num_epochs': 14}\n",
      "Best score: 0.8202\n",
      "Epoch 1/14, Loss: 0.0788\n",
      "Epoch 2/14, Loss: 0.3042\n",
      "Epoch 3/14, Loss: 0.5500\n",
      "Epoch 4/14, Loss: 0.1436\n",
      "Epoch 5/14, Loss: 0.2163\n",
      "Epoch 6/14, Loss: 0.0419\n",
      "Epoch 7/14, Loss: 0.0436\n",
      "Epoch 8/14, Loss: 0.0427\n",
      "Epoch 9/14, Loss: 0.0407\n",
      "Epoch 10/14, Loss: 0.0434\n",
      "Epoch 11/14, Loss: 0.0928\n",
      "Epoch 12/14, Loss: 0.0404\n",
      "Epoch 13/14, Loss: 0.2447\n",
      "Epoch 14/14, Loss: 0.3591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.55      0.71        31\n",
      "     Class 1       0.68      0.76      0.72        17\n",
      "     Class 2       0.42      0.62      0.50        16\n",
      "     Class 3       0.69      0.58      0.63        43\n",
      "     Class 4       0.91      0.96      0.93       193\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.74      0.70      0.70       300\n",
      "weighted avg       0.85      0.83      0.83       300\n",
      "\n",
      "Ordinal accuracy of the network on the test data: 96%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def ordinal_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    total_count = len(y_true)\n",
    "    accurate_count = sum(\n",
    "        1\n",
    "        for true_label, pred_label in zip(y_true, y_pred)\n",
    "        if pred_label in [true_label, true_label - 1, true_label + 1]\n",
    "    )\n",
    "    return accurate_count / total_count\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "datafile_path = \"../data/fine_food_reviews_fine_tuned_e5_small_v2_1k.parquet\"\n",
    "df = pd.read_parquet(datafile_path)\n",
    "\n",
    "embedding_dim = np.array(list(df.embedding.values)).shape[1]\n",
    "print(f\"Shape of embeddings in the dataframe: {embedding_dim}\")\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values),\n",
    "    df.Score - 1,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Convert the problem to three classes (very bad, bad, neutral-good)\n",
    "# y_train = y_train.apply(lambda x: 0 if x < 3 else 1 if x == 3 else 2)\n",
    "# y_test = y_test.apply(lambda x: 0 if x < 3 else 1 if x == 3 else 2)\n",
    "\n",
    "n_classes = len(y_train.unique())\n",
    "\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embeddings[idx].clone().detach().float()\n",
    "        label = self.labels[idx].clone().detach().long()\n",
    "        return embedding, label\n",
    "\n",
    "\n",
    "# Convert train and test splits to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach()\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long).clone().detach()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).clone().detach()\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long).clone().detach()\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = ReviewsDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = ReviewsDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": embedding_dim,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"num_hidden_1\": 256,\n",
    "    \"num_hidden_2\": 128,\n",
    "    \"num_hidden_3\": 64,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"num_epochs\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        dim_feedforward,\n",
    "        output_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer_encoder(x.unsqueeze(1))  # Add sequence dimension\n",
    "        x = self.fc(x.squeeze(1))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    nhead = trial.suggest_int(\"nhead\", 1, 8)\n",
    "    d_model = trial.suggest_int(\"d_model\", nhead * 8, nhead * 64, step=nhead * 8)\n",
    "    num_encoder_layers = trial.suggest_int(\"num_encoder_layers\", 1, 6)\n",
    "    dim_feedforward = trial.suggest_int(\"dim_feedforward\", 128, 1024)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 32, 64, 128])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 10, 50)\n",
    "\n",
    "    # Create DataLoader objects with the new batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create model\n",
    "    model = TransformerModel(\n",
    "        input_size=embedding_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        output_size=n_classes,\n",
    "    ).to(params[\"device\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, targets in train_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features = features.to(params[\"device\"])\n",
    "            targets = targets.to(params[\"device\"])\n",
    "\n",
    "            logits = model(features)\n",
    "            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    return f1_score(all_targets, all_preds, average=\"weighted\")\n",
    "\n",
    "\n",
    "# Run the Optuna optimization\n",
    "tpe_sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=tpe_sampler)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_trial.value\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "\n",
    "# Update the params dictionary with the best hyperparameters\n",
    "params.update(best_params)\n",
    "\n",
    "# Create DataLoader objects with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Create and train the final model with the best hyperparameters\n",
    "model = TransformerModel(\n",
    "    input_size=params[\"input_dim\"],\n",
    "    d_model=params[\"d_model\"],\n",
    "    nhead=params[\"nhead\"],\n",
    "    num_encoder_layers=params[\"num_encoder_layers\"],\n",
    "    dim_feedforward=params[\"dim_feedforward\"],\n",
    "    output_size=params[\"n_classes\"],\n",
    ").to(params[\"device\"])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    for features, targets in train_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(params[\"device\"])\n",
    "        targets = targets.to(params[\"device\"])\n",
    "\n",
    "        logits = model(features)\n",
    "        preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\n",
    "    classification_report(\n",
    "        all_targets, all_preds, target_names=[f\"Class {i}\" for i in range(n_classes)]\n",
    "    )\n",
    ")\n",
    "\n",
    "ordinal_accuracy_score = ordinal_accuracy(np.array(all_targets), np.array(all_preds))\n",
    "print(f\"Ordinal accuracy of the network on the test data: {ordinal_accuracy_score:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model analysis\n",
    "from scipy.special import expit as sigmoid, softmax\n",
    "\n",
    "def label_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    z = softmax(logits, axis=1)\n",
    "    return z / z.sum(axis=1, keepdims=1)\n",
    "\n",
    "def probas_from_logits(logits):\n",
    "    \"\"\"Converts logits to class labels.\"\"\"\n",
    "    z = softmax(logits, axis=1)\n",
    "    softm_ = z / z.sum(axis=1, keepdims=1)\n",
    "    # probas_cum = np.cumprod(probas, axis=1)\n",
    "    return (softm_ > 0.5).astype(int)\n",
    "\n",
    "logits_df = pd.DataFrame(all_logits)\n",
    "preds_df = pd.DataFrame(label_from_logits(all_logits))\n",
    "probas_df = pd.DataFrame(probas_from_logits(all_logits))\n",
    "labels_df = pd.DataFrame({'True Label': all_targets, 'Predicted Label': all_preds})\n",
    "\n",
    "slice_window = slice(160, 170)\n",
    "df_to_show = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(probas_df[slice_window]), \n",
    "        labels_df[slice_window]['True Label'], \n",
    "        labels_df[slice_window]['Predicted Label']\n",
    "    ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  True Label  Predicted Label\n",
       "160  0  0  0  0  1           4                4\n",
       "161  0  0  1  0  0           1                2\n",
       "162  0  0  0  0  1           4                4\n",
       "163  0  0  0  0  1           4                4\n",
       "164  0  0  0  0  1           4                4\n",
       "165  1  0  0  0  0           0                0\n",
       "166  0  1  0  0  0           0                1\n",
       "167  0  0  0  0  1           3                4\n",
       "168  0  0  0  1  0           3                3\n",
       "169  0  0  0  0  1           4                4"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lr-focal-loss-Opjerf94-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
