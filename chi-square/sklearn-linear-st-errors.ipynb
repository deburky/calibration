{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Standard Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: 0.9136, b1: 2.0069\n",
      "SE b0: 0.091, SE b1: 0.016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate some linear data\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1)  # Shape (100, 1)\n",
    "y = 2 * x + 1 + np.random.randn(100, 1) * 0.5  # Shape (100, 1)\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.slope = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Add a column of ones to X for the intercept\n",
    "        X = np.hstack([np.ones((x.shape[0], 1)), x])  # Shape (100, 2)\n",
    "        # Compute parameters using the normal equation\n",
    "        params = np.linalg.inv(X.T @ X) @ X.T @ y  # Shape (2, 1)\n",
    "        # Separate slope and intercept\n",
    "        self.intercept = params[0, 0]  # Intercept is the first parameter\n",
    "        self.slope = params[1, 0]  # Slope is the second parameter\n",
    "\n",
    "    def calculate_se(self, x, y):\n",
    "        # Add a column of ones to X for the intercept\n",
    "        X = np.hstack([np.ones((x.shape[0], 1)), x])  # Shape (n, 2)\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = y.flatten() - self.predict(x).flatten()\n",
    "        \n",
    "        # Residual Sum of Squares (RSS)\n",
    "        rss = np.sum(residuals ** 2)\n",
    "        \n",
    "        # Degrees of freedom: n - p\n",
    "        n, p = X.shape\n",
    "        variance = rss / (n - p)\n",
    "        \n",
    "        # Variance-Covariance matrix of coefficients\n",
    "        cov_matrix = variance * np.linalg.inv(X.T @ X)\n",
    "        \n",
    "        # Standard errors are the square root of the diagonal elements\n",
    "        return np.sqrt(np.diag(cov_matrix))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Add a column of ones to X for the intercept\n",
    "        X = np.hstack([np.ones((x.shape[0], 1)), x])  # Shape (n, 2)\n",
    "        return X @ np.array([self.intercept, self.slope]).flatten()\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(x)\n",
    "standard_errors = model.calculate_se(x, y)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(f\"b0: {model.intercept:.4f}, b1: {model.slope:.4f}\")\n",
    "print(f\"SE b0: {standard_errors[0]:.3f}, SE b1: {standard_errors[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: [0.91359357], b1: [[2.00689663]]\n",
      "SE(b0): 0.091, SE(b1): 0.016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "model_sk = LinearRegression()\n",
    "model_sk.fit(x, y)\n",
    "\n",
    "predictions = model_sk.predict(x)\n",
    "\n",
    "def calculate_sk_st_errors(x, y):\n",
    "   X = np.column_stack((np.ones(len(x)), x))\n",
    "   rss = np.sum((y - model_sk.predict(x))**2)\n",
    "   n, p = X.shape\n",
    "   variance = rss / (n - p)\n",
    "   cov_matrix = variance * np.linalg.inv(np.dot(X.T, X))\n",
    "   se_b0 = np.sqrt(cov_matrix[0, 0])\n",
    "   se_b1 = np.sqrt(cov_matrix[1, 1])\n",
    "   return se_b0, se_b1\n",
    "\n",
    "se_b0, se_b1 = calculate_sk_st_errors(x, y)\n",
    "print(f\"b0: {model_sk.intercept_}, b1: {model_sk.coef_}\")\n",
    "print(f\"SE(b0): {se_b0:.3f}, SE(b1): {se_b1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.994\n",
      "Model:                            OLS   Adj. R-squared:                  0.994\n",
      "Method:                 Least Squares   F-statistic:                 1.647e+04\n",
      "Date:                Mon, 13 Jan 2025   Prob (F-statistic):          5.37e-111\n",
      "Time:                        12:24:24   Log-Likelihood:                -62.345\n",
      "No. Observations:                 100   AIC:                             128.7\n",
      "Df Residuals:                      98   BIC:                             133.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.9136      0.091     10.094      0.000       0.734       1.093\n",
      "x1             2.0069      0.016    128.341      0.000       1.976       2.038\n",
      "==============================================================================\n",
      "Omnibus:                        0.521   Durbin-Watson:                   2.042\n",
      "Prob(Omnibus):                  0.771   Jarque-Bera (JB):                0.518\n",
      "Skew:                          -0.167   Prob(JB):                        0.772\n",
      "Kurtosis:                       2.885   Cond. No.                         11.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(x)\n",
    "model_sm = sm.OLS(y, X).fit()\n",
    "\n",
    "print(model_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 11 iterations.\n",
      "Weights: [-10.1445658   42.48382473]\n",
      "Standard errors: [ 3.21784182 13.21419489]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + 0.1 * np.random.randn(100, 1)\n",
    "y = (y > 1.5).astype(int).flatten()\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, max_iter=100):\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Add a column of ones to X for the intercept\n",
    "        X = np.hstack([np.ones((x.shape[0], 1)), x])  # Shape (n, p+1)\n",
    "        self.weights = np.zeros(X.shape[1])  # Initialize weights (p+1,)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Predictions (sigmoid function)\n",
    "            predictions = sigmoid(X @ self.weights)\n",
    "\n",
    "            # Gradient (score vector)\n",
    "            score = X.T @ (y - predictions)\n",
    "\n",
    "            # Diagonal of weights for Hessian\n",
    "            W_diag = (predictions * (1 - predictions)).ravel()\n",
    "            W = np.diag(W_diag)\n",
    "\n",
    "            # Hessian (Information matrix)\n",
    "            information = X.T @ W @ X\n",
    "\n",
    "            # Update weights using Newton-Raphson method\n",
    "            beta_new = self.weights + score @ np.linalg.inv(information)\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(beta_new - self.weights) < self.epsilon:\n",
    "                self.weights = beta_new\n",
    "                print(f\"Converged in {iteration + 1} iterations.\")\n",
    "                break\n",
    "\n",
    "            self.weights = beta_new\n",
    "\n",
    "    def calculate_st_errors(self, x):\n",
    "        # Predictions\n",
    "        X = np.hstack([np.ones((x.shape[0], 1)), x])  # Shape (n, p+1)\n",
    "        information = X.T @ np.diag((self.predict_proba(x) * (1 - self.predict_proba(x))).ravel()) @ X\n",
    "        # Standard errors\n",
    "        return np.sqrt(np.diag(np.linalg.inv(information)))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Add a column of ones to X for the intercept\n",
    "        X = np.hstack([np.ones((x.shape[0], 1)), x])  # Shape (n, p+1)\n",
    "        # Return probabilities\n",
    "        return sigmoid(X @ self.weights)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Convert probabilities to binary predictions\n",
    "        return (self.predict_proba(x) >= 0.5).astype(int)\n",
    "\n",
    "# Instantiate and fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Print learned weights\n",
    "print(f\"Weights: {model.weights}\")\n",
    "\n",
    "st_errors = model.calculate_st_errors(x)\n",
    "print(f\"Standard errors: {st_errors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: [-9.97027934], b1: [[41.73871733]]\n",
      "SE(b0): 3.141, SE(b1): 12.884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_sk = LogisticRegression(penalty=None, solver='newton-cg')\n",
    "model_sk.fit(x, y)\n",
    "\n",
    "def calculate_sk_st_errors(model, x):\n",
    "   X = np.column_stack((np.ones(len(x)), x))\n",
    "   predictions = model.predict_proba(x)[:, 1]\n",
    "   W = np.diag(predictions * (1 - predictions))\n",
    "   information = X.T @ W @ X\n",
    "   return np.sqrt(np.diag(np.linalg.inv(information)))\n",
    "\n",
    "print(f\"b0: {model_sk.intercept_}, b1: {model_sk.coef_}\")\n",
    "\n",
    "se_b0, se_b1 = calculate_sk_st_errors(model_sk, x)\n",
    "print(f\"SE(b0): {se_b0:.3f}, SE(b1): {se_b1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.069751\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       98\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Mon, 13 Jan 2025   Pseudo R-squ.:                  0.8842\n",
      "Time:                        12:52:15   Log-Likelihood:                -6.9751\n",
      "converged:                       True   LL-Null:                       -60.215\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.787e-25\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -10.1446      3.218     -3.153      0.002     -16.451      -3.838\n",
      "x1            42.4838     13.214      3.215      0.001      16.584      68.383\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.53 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "model_sm = sm.Logit(y, X).fit()\n",
    "print(model_sm.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-project-nVfOrddR-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
